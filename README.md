# Stock Market Trading & Portfolio Performance Analytics Platform

An end-to-end data engineering and analytics platform designed to process, analyze, and curate stock market and portfolio performance data using scalable, production-grade architectures.

![Airflow](https://img.shields.io/badge/Airflow-0078D4?style=for-the-badge&logo=microsoft-azure&logoColor=white)
![Databricks](https://img.shields.io/badge/Databricks-FF3621?style=for-the-badge&logo=databricks&logoColor=white)
![Spark](https://img.shields.io/badge/Apache_Spark-E25A1C?style=for-the-badge&logo=apachespark&logoColor=white)
![Power BI](https://img.shields.io/badge/Power_BI-F2C811?style=for-the-badge&logo=powerbi&logoColor=black)
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)


---

## ğŸ“– Project Overview

This project focuses on building a robust stock market analytics pipeline that transforms raw financial time-series data into curated, analytics-ready datasets. The platform is designed following modern data engineering best practices and supports downstream reporting, portfolio evaluation, and market insight generation.

The solution emphasizes scalability, data quality, and modularity, making it suitable for real-world financial analytics use cases.

---

## ğŸ¯ Business Objectives

- Analyze historical stock price movements across multiple companies  
- Evaluate portfolio performance and investment exposure  
- Generate return, trend, and risk-related financial metrics  
- Enable decision-ready datasets for dashboards and analytics  

---

## ğŸ› ï¸ Technology Stack

- **Python** â€“ Data ingestion and validation  
- **Pandas** â€“ Time-series feature engineering  
- **Apache Spark (PySpark)** â€“ Scalable data processing  
- **Delta Lake** â€“ Reliable storage with ACID compliance  
- **Azure Databricks** â€“ Distributed analytics platform  
- **Apache Airflow** â€“ Workflow orchestration  
- **Docker & Docker Compose** â€“ Containerized Airflow setup  
- **Power BI** â€“ Data visualization and reporting  

---

## ğŸ—ï¸ High-Level Architecture

```text
Raw CSV Data
     â†“
Python Ingestion & Validation
     â†“
Bronze Layer (Delta Tables)
     â†“
Silver Layer (Cleaned & Enriched)
     â†“
Gold Layer (Aggregated Business Metrics)
     â†“
Power BI Dashboards
````

---

## â±ï¸ Workflow Orchestration (Apache Airflow)

Apache Airflow is used to orchestrate and trigger Databricks Serverless jobs responsible for executing the ETL pipeline.

### Key Highlights:

* Dockerized Airflow setup using `docker-compose`
* LocalExecutor with Postgres metadata database
* DAG triggers Databricks Serverless jobs via `DatabricksRunNowOperator`
* Supports retry logic and failure handling
* Enables modular and automated pipeline execution

---

## ğŸ“‚ Repository Structure

```text
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ airflow-dags/
â”‚   â”‚   â””â”€â”€ stocks.py
â”‚   â”œâ”€â”€ airflow-logs/
â”‚   â”œâ”€â”€ airflow-plugins/
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ bronze_layer.ipynb
â”‚   â”œâ”€â”€ silver_layer.ipynb
â”‚   â””â”€â”€ gold_layer.ipynb
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ stocks/
â”‚   â”‚   â”‚   â”œâ”€â”€ AAPL.csv
â”‚   â”‚   â”‚   â”œâ”€â”€ GOOGL.csv
â”‚   â”‚   â”‚   â”œâ”€â”€ JPM.csv
â”‚   â”‚   â”‚   â””â”€â”€ MSFT.csv
â”‚   â”‚   â””â”€â”€ portfolio/
â”‚   â”‚       â””â”€â”€ portfolio_transactions.csv
â”‚   â”‚
â”‚   â””â”€â”€ processed/
â”‚       â”œâ”€â”€ clean_stock_data.csv
â”‚       â””â”€â”€ stock_analytics_pandas.csv
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â””â”€â”€ ingest_stock_data.py
â”‚   â”‚
â”‚   â”œâ”€â”€ transformation/
â”‚   â”‚   â””â”€â”€ pandas_time_series.py
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ generate_datasets.py
â”‚
â”œâ”€â”€ databricks/               # Databricks notebooks for Bronzeâ€“Silverâ€“Gold layers
â”œâ”€â”€ powerbi/
â”‚   â””â”€â”€ Stock Market Trading & Portfolio Performance.pbix
â”œâ”€â”€ airflow/                  # Airflow DAGs and Docker setup
â”œâ”€â”€ README.md

```
---
## ğŸ”¹ Logging, Monitoring & Observability

Execution-level logging and monitoring are handled by Apache Airflow and Databricks, which automatically capture task execution details, failures, and retry events, without requiring custom logging logic in the application code.

Airflow provides detailed logs for DAG parsing, task execution, retries, and failures, while Databricks captures job execution logs, Spark driver logs, and executor logs through its managed runtime environment.

---

## ğŸ“Š Analytics & Metrics

The platform computes and exposes the following analytics:

* Daily and cumulative returns
* Short-term and long-term moving averages
* Volatility and risk indicators
* Stock-wise performance trends
* Portfolio-level investment exposure

---

## ğŸ“ˆ Dashboards

The curated Gold-layer datasets are used to build interactive dashboards, including:

* **Executive Market Overview**
* **Portfolio Performance Dashboard**
* **Risk & Market Insights Dashboard**


---

## ğŸš€ Future Enhancements

* Real-time stock data ingestion using APIs
* Advanced risk modeling and factor analysis
* Machine learning-based price forecasting
* Automated data quality monitoring
* CI/CD integration for pipelines

---

## ğŸ‘¤ Author

**Gayathri Pavushetty**
Stock Market Trading & Portfolio Performance Analytics Platform
